{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the curated data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First step create the text file consisting of columns PMID, title and abstract and save it in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "YOUR_FILE = ''\n",
    "dd = pd.read_csv(YOUR_FILE, sep='\\t')\n",
    "# Combine title and abstract\n",
    "dd['text'] = dd['title'] + \" \" + dd['abstract']\n",
    "labels = dd['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "download_dir = './'\n",
    "\n",
    "nltk.download('stopwords', download_dir=download_dir)\n",
    "nltk.download('punkt_tab', download_dir=download_dir) # THIS IS WHERE THE PROBLEM WAS\n",
    "# set the right path so it looks in the right place\n",
    "nltk.data.path = [download_dir]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "def preprocess_text(text):\n",
    "    text = text.lower() # make all text lower case\n",
    "    tokens = text.split() # split text into individual words\n",
    "    tokens = [word.strip(string.punctuation) for word in tokens] # remove punctuations\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words] # ony keep words that are not in stop_words\n",
    "    processed_text = ' '.join(filtered_tokens) # rejoin the tokens into a string\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the preprocessing function on the full text column\n",
    "dd['processed_text'] = dd['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the processing worked\n",
    "sample_texts = dd['processed_text'].head(2)\n",
    "\n",
    "for text in sample_texts:\n",
    "    words = text.split()  # Split text into individual words\n",
    "    print(text)\n",
    "    print(\"Contains 'the':\", 'the' in words)\n",
    "    print(\"Contains 'is':\", 'is' in words)\n",
    "    print(\"Contains 'and':\", 'and' in words)\n",
    "    print(\"----------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = dd['processed_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "documents = dd['processed_text'].tolist()  # Use your processed text data\n",
    "\n",
    "# Initialize BERTopic with zero-shot topics\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,  # Use a model that fits your setup\n",
    "    min_topic_size=5,  # Adjust if needed\n",
    "    representation_model=KeyBERTInspired()\n",
    ")\n",
    "\n",
    "# Fit the model on the documents\n",
    "topics, probabilities = topic_model.fit_transform(documents)\n",
    "\n",
    "print(topic_model.get_topic_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_document_info(documents).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the topics to the original dataframe\n",
    "dd['topic'] = topics\n",
    "dd['topic_probability'] = probabilities\n",
    "\n",
    "dd.to_csv('document_topics_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
